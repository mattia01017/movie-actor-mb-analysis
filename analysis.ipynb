{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "! kaggle datasets download -f actors.csv gsimonx37/letterboxd\n",
    "! unzip actors.csv.zip\n",
    "! rm actors.csv.zip\n",
    "\n",
    "# get whole repo if running in google colab\n",
    "! git clone https://github.com/mattia01017/movie-actor-mb-analysis\n",
    "! pip install -r movie-actor-mb-analysis/requirements.txt\n",
    "\n",
    "# setup Spark\n",
    "import os\n",
    "import findspark\n",
    "! apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "! wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
    "! tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
    "! pip install -q findspark\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "findspark.init(\"spark-3.1.1-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pympler.asizeof import asizeof\n",
    "from typing import Iterable\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations, count\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import RDD\n",
    "import gc\n",
    "\n",
    "CHUNKS = 5\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"movie-actor-mb-analysis\")\\\n",
    "    .config(\"spark.default.parallelism\", str(CHUNKS))\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market-basket analysis of Letterboxd dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Starting from a csv table that associate film identifiers to actors, we generate the list of baskets and save it to disk. The list is sorted by movie ID to obtain a predictable order and is not strictly needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"actors.csv\", header=True, sep=\",\", mode=\"DROPMALFORMED\")\n",
    "df.rdd\\\n",
    "    .map(lambda x: (x[\"id\"], x[\"name\"]))\\\n",
    "    .groupByKey()\\\n",
    "    .sortByKey()\\\n",
    "    .map(lambda x: json.dumps(list(x[1])))\\\n",
    "    .saveAsTextFile(\"baskets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an iterator that implement a lazy loading of baskets from files. In this way, we can load in memory a basket at a time instead of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baskets:\n",
    "    def __init__(self, parts_path) -> None:\n",
    "        self.parts_path = parts_path\n",
    "        \n",
    "    def _file_name(self):\n",
    "        return \"{0}/part-{1:0>5}\".format(self.parts_path, self.part)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.part = 0\n",
    "        self.file = open(self._file_name())\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            line = next(self.file)\n",
    "        except StopIteration: \n",
    "            self.file.close()\n",
    "            self.part += 1\n",
    "            try:\n",
    "                self.file = open(self._file_name())\n",
    "                return self.__next__()\n",
    "            except FileNotFoundError:\n",
    "                raise StopIteration\n",
    "        return tuple(json.loads(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "For the analysis, the Savasere, Omiecinski and Navathe (SON) algorithm will be implemented using the Park, Cheng and Yu (PCY) algorithm for retrieving frequent itemsets in the chunks.\n",
    "\n",
    "### PCY\n",
    "\n",
    "First, a very simple class representing a bitmap useful for the PCY algorithm implementation is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bitmap:\n",
    "    def __init__(self, bits_arr: list) -> None:\n",
    "        self.bytes = np.packbits(bits_arr, bitorder=\"little\")\n",
    "\n",
    "    def get(self, index: int) -> bool:\n",
    "        return bool(self.bytes[index // 8] & pow(2, index % 8))\n",
    "\n",
    "    def set(self, index: int):\n",
    "        self.bytes[index // 8] |= pow(2, index % 8)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \" \".join([\"{0:08b}\".format(b) for b in self.bytes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, the PCY algorithm can be implemented. The garbage collector is manually triggered for deleting from memory the counters immediately after the bitmap creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_t(itemset: tuple) -> int:\n",
    "    \"\"\"hash tuple ignoring order\"\"\"\n",
    "    return reduce(lambda p, c: p ^ hash(c), itemset, 0)\n",
    "\n",
    "\n",
    "def apriori(\n",
    "    baskets: Iterable[tuple[str]],\n",
    "    threshold: int,\n",
    "    freq_items: list[str],\n",
    "    freq_couples: list[tuple],\n",
    "    iset_len_limit: int | None = None\n",
    ") -> list[tuple]:\n",
    "    freq_itemsets = freq_couples\n",
    "    sizes_iter = iset_len_limit and range(3, iset_len_limit+1) or count(3)\n",
    "    for s in sizes_iter:\n",
    "        counters = defaultdict(int)\n",
    "        for basket in baskets:\n",
    "            filtered_basket = (item for item in basket if item in freq_items)\n",
    "            filtered_basket = set(sum(\n",
    "                (list(itemset) for itemset in combinations(filtered_basket, s-1) if tuple(sorted(itemset)) in freq_itemsets), []\n",
    "            ))\n",
    "            for itemset in combinations(filtered_basket, s):\n",
    "                counters[tuple(sorted(itemset))] += 1\n",
    "        new_frequent = [itemset for itemset, count in counters.items() if count > threshold]\n",
    "        if len(new_frequent) == 0: break\n",
    "        freq_itemsets.extend(new_frequent)\n",
    "    \n",
    "    return freq_itemsets\n",
    "\n",
    "\n",
    "def PCY(\n",
    "    baskets: Iterable[tuple[str]],\n",
    "    threshold: int,\n",
    "    buckets: int,\n",
    "    iset_len_limit: int | None = None\n",
    ") -> list[tuple]:  \n",
    "    item_counts = Counter()\n",
    "    itemset_counts = np.zeros(buckets, dtype=np.uint32)\n",
    "\n",
    "    for basket in baskets:\n",
    "        for item in basket:\n",
    "            item_counts[item] += 1\n",
    "        for itemset in combinations(basket, 2):\n",
    "            itemset_counts[hash_t(itemset) % buckets] += 1\n",
    "\n",
    "    freq_items = [item for item, count in item_counts.items() if count > threshold]\n",
    "    del item_counts\n",
    "    gc.collect()\n",
    "\n",
    "    bitmap = Bitmap([count > threshold for count in itemset_counts])\n",
    "    del itemset_counts\n",
    "    gc.collect()\n",
    "    \n",
    "    freq_couples = [\n",
    "        tuple(sorted(itemset))\n",
    "        for itemset in combinations(freq_items, 2)\n",
    "        if bitmap.get(hash_t(itemset) % buckets)\n",
    "    ]\n",
    "    \n",
    "    return apriori(baskets, threshold, freq_items, freq_couples, iset_len_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, the itemsets with cardinality larger than 2 are obtained using the apriori algorithm, as a low number of those itemset is expected to be frequent.\n",
    "\n",
    "PCY alone can be used to retrieve frequent itemset (buckets) using a single node for computation. To avoid long execution times, in this section only frequent couples will be computed. Unsetting the `iset_len_limit` optional parameter in the cell below (or setting it to an higher value) will force the search to look for frequent itemsets with higher cardinality. The next cell should take around 5 minutes in a Google Colab CPU runtime.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCY(Baskets(\"baskets\"), 170, int(1e6), iset_len_limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result contains also false positive couples, that is infrequent couples put in a frequent buckets. To remove false positives, another pass would be needed. The problem is best delved during SON implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SON\n",
    "\n",
    "Execution times can be improved by using SON, parallelizing the execution of PCY on a number of chunks and combining the results. The Apache Spark framework is used for the implementation of the SON algorithm. \n",
    "\n",
    "We start by loading the basket files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"baskets\")\n",
    "baskets: RDD = df.rdd.map(lambda row: tuple(json.loads(row.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "634300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baskets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory usage\n",
    "\n",
    "The objective is to have the maximum memory usage without swapping and thus thrashing. The main elements to store in memory are:\n",
    "- The hash table of item counters\n",
    "- The array of bucket counters\n",
    "- The bitmap of frequent buckets\n",
    "\n",
    "The memory usage of the bitmap and the array of counters is easy to predict given the size, more tricky is doing it for the hash table of counters. For this purpose, we use a tool for observing memory behaviour of Python objects, namely Pympler. The `asizeof` method return an approximation of the memory usage of an object.\n",
    "\n",
    "We measure the size of the `Counter` object after counting all items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.977 MB\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(baskets.flatMap(lambda x: x).collect())\n",
    "print(\"{0:.3f} MB\".format(asizeof(counter) / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can assume that a single node won't use more than 200 MB for storing the item counters. The remaining space can be used to store the bucket counters. Assuming we want to use up to 2 GB of memory for each computing node, we can use a number of buckets with 32-bit unsigned integer counters equal to:\n",
    "$$\n",
    "\\frac{2 \\cdot 10^9 \\text{ B} - 2 \\cdot 10^8 \\text{ B}}{4 \\text{ B}} = 4.5 \\cdot 10^8 \\text{ buckets}\n",
    "$$\n",
    "\n",
    "while the bitmap will occupy $1/32$ of the space, that is $56 \\text{ MB}$.\n",
    "\n",
    "The last parameters to tune are the thresholds for labelling an itemset as frequent. This will be chosen experimentally in a way to obtain an output of reasonable size, say around 50 itemsets in the whole basket list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-reduce implementation\n",
    "\n",
    "To avoid long times in resource constrained environments, only a randomly sampled list of baskets is used during the analysis. For the same reason, the used number of bucket will be smaller than the one computed above.\n",
    "\n",
    "The result of the analysis on the original dataset using $10^7$ buckets and a threshold of 100 is reported in the `frequent_itemset.json` file in the repository. The next cell shows an execution on a much smaller set of baskets, considering only couples. To search from larger baskets and to retrieve itemsets with larger cardinality, simply increase respectively the `SAMPLE_FRACTION` and `ISET_LEN_LIMIT` constants, eventually setting to `None` the latter to remove the cardinality limit.\n",
    "\n",
    "The cell below takes around 8 minutes to generate a result in a Google Colab CPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKETS = int(1e7)\n",
    "THRESHOLD = 45\n",
    "ISET_LEN_LIMIT = 2\n",
    "SAMPLE_FRACTION = .25\n",
    "\n",
    "if SAMPLE_FRACTION < 1:\n",
    "    baskets = baskets.sample(False, SAMPLE_FRACTION, 2)\n",
    "\n",
    "def count_occurrences(\n",
    "    baskets: Iterable, \n",
    "    candidates: Iterable, \n",
    "    iset_len_limit: int | None = None\n",
    "):\n",
    "    out = {tuple(sorted(k)):0 for k in candidates}\n",
    "    for basket in baskets:\n",
    "        sizes_iter = iset_len_limit and range(2, iset_len_limit+1) or count(2)\n",
    "        for size in sizes_iter:\n",
    "            exit = True\n",
    "            for itemset in combinations(basket, size):\n",
    "                itemset = tuple(sorted(itemset))\n",
    "                if itemset in out:\n",
    "                    exit = False\n",
    "                    out[itemset] += 1\n",
    "            if exit: break        \n",
    "    return out.items()\n",
    "\n",
    "def SON(baskets: RDD, threshold: int, chunks: int, buckets: int) -> list[tuple]:\n",
    "    candidates = baskets\\\n",
    "        .mapPartitions(lambda chunk: PCY(list(chunk), threshold // chunks, buckets, ISET_LEN_LIMIT))\\\n",
    "        .distinct()\\\n",
    "        .collect()\n",
    "    frequent_itemsets = baskets\\\n",
    "        .mapPartitions(lambda chunk: count_occurrences(chunk, candidates, ISET_LEN_LIMIT))\\\n",
    "        .reduceByKey(lambda a, b: a + b)\\\n",
    "        .filter(lambda x:  x[1] > THRESHOLD)\\\n",
    "        .collect()\n",
    "    frequent_itemsets.sort(key=lambda x: -x[1])\n",
    "    return frequent_itemsets\n",
    "\n",
    "frequent_itemsets = SON(baskets, THRESHOLD, CHUNKS, BUCKETS)\n",
    "\n",
    "for x in frequent_itemsets[:10]: print(x)\n",
    "with open(\"frequent_itemsets.json\", \"w\") as f:\n",
    "    json.dump([{\"set\": s[0], \"count\": s[1]} for s in frequent_itemsets], f, indent=2)\n",
    "\n",
    "print(\"Number of frequent itemsets:\", len(frequent_itemsets))\n",
    "print(\"Whole list of frequent itemsets saved in 'frequent_itemsets.json'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
