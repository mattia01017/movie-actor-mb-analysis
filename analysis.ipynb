{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "! curl -L -o actors.csv.zip \"https://www.kaggle.com/api/v1/datasets/download/gsimonx37/letterboxd/actors.csv\"\n",
    "! unzip actors.csv.zip\n",
    "! rm actors.csv.zip\n",
    "\n",
    "# get whole repo if running in google colab\n",
    "! git clone https://github.com/mattia01017/movie-actor-mb-analysis\n",
    "! pip install -r movie-actor-mb-analysis/requirements.txt\n",
    "\n",
    "# setup Spark\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pympler.asizeof import asizeof\n",
    "from typing import Iterable\n",
    "from collections import Counter\n",
    "from itertools import combinations, count\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import RDD\n",
    "import gc\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"movie-actor-mb-analysis\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market-basket analysis of Letterboxd dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Starting from a csv table that associate film identifiers to actors, we generate the list of baskets and save it to disk. The list is sorted by movie ID to obtain a predictable order and is not strictly needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"actors.csv\", header=True, sep=\",\", mode=\"DROPMALFORMED\")\n",
    "df.rdd\\\n",
    "    .map(lambda x: (x[\"id\"], x[\"name\"]))\\\n",
    "    .groupByKey()\\\n",
    "    .sortByKey()\\\n",
    "    .map(lambda x: json.dumps(list(x[1])))\\\n",
    "    .saveAsTextFile(\"baskets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an iterator that implement a lazy loading of baskets from files. In this way, we can load in memory a basket at a time instead of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baskets:\n",
    "    def __init__(self, parts_path) -> None:\n",
    "        self.parts_path = parts_path\n",
    "        \n",
    "    def _file_name(self):\n",
    "        return \"{0}/part-{1:0>5}\".format(self.parts_path, self.part)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.part = 0\n",
    "        self.file = open(self._file_name())\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            line = next(self.file)\n",
    "        except StopIteration: \n",
    "            self.file.close()\n",
    "            self.part += 1\n",
    "            try:\n",
    "                self.file = open(self._file_name())\n",
    "                return next(self)\n",
    "            except FileNotFoundError:\n",
    "                raise StopIteration\n",
    "        return tuple(json.loads(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "For the analysis, the Savasere, Omiecinski and Navathe (SON) algorithm will be implemented using the Park, Cheng and Yu (PCY) algorithm for retrieving frequent itemsets in the chunks.\n",
    "\n",
    "### PCY\n",
    "\n",
    "First, a very simple class representing a bitmap useful for the PCY algorithm implementation is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bitmap:\n",
    "    def __init__(self, bits_arr: list) -> None:\n",
    "        self.bytes = np.packbits(bits_arr, bitorder=\"little\")\n",
    "\n",
    "    def get(self, index: int) -> bool:\n",
    "        return bool(self.bytes[index // 8] & pow(2, index % 8))\n",
    "\n",
    "    def set(self, index: int):\n",
    "        self.bytes[index // 8] |= pow(2, index % 8)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \" \".join([\"{0:08b}\".format(b) for b in self.bytes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, the PCY algorithm can be implemented. The garbage collector is manually triggered for deleting from memory the counters immediately after the bitmap creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_t(itemset: tuple) -> int:\n",
    "    \"\"\"hash tuple ignoring order\"\"\"\n",
    "    return reduce(lambda p, c: p ^ hash(c), itemset, 0)\n",
    "\n",
    "\n",
    "def _get_candidate_items(\n",
    "    freq_itemset: frozenset, basket: tuple, last_freq_iset: list[tuple], s: int\n",
    "):\n",
    "    \"\"\"This helper function take the frequent items in a basket, if those appear at least s-1 times\n",
    "    in subsets of size s-1, then they must be in a candidate subset of size s\"\"\"\n",
    "    basket_freq_items = freq_itemset.intersection(basket)\n",
    "    items_in_freq_subset = (\n",
    "        item for iset in last_freq_iset for item in iset if item in basket_freq_items\n",
    "    )\n",
    "    return [item for item, cnt in Counter(items_in_freq_subset).items() if cnt >= s - 1]\n",
    "\n",
    "\n",
    "def apriori(\n",
    "    baskets: Iterable[tuple[str]],\n",
    "    threshold: int,\n",
    "    freq_items: list[str],\n",
    "    freq_couples: list[tuple],\n",
    "    iset_len_limit: int | None = None,\n",
    ") -> list[tuple]:\n",
    "    \"\"\"Apriori algorithm starting from frequent couples\n",
    "\n",
    "    Args:\n",
    "        baskets (Iterable[tuple[str]]): the baskets\n",
    "        threshold (int): the threshold over which an itemset is frequent\n",
    "        freq_items (list[str]): items found to be frequent, assumed to be non descending sorted\n",
    "        freq_couples (set[tuple]): couples found to be frequent\n",
    "        iset_len_limit (int | None, optional): the maximum cardinality of itemsets to consider. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple]: all the frequent itemsets, including the couples\n",
    "    \"\"\"\n",
    "    freq_items_set = frozenset(freq_items)\n",
    "    result = freq_couples\n",
    "    last_freq_iset = freq_couples\n",
    "    sizes = iset_len_limit and range(3, iset_len_limit + 1) or count(3)\n",
    "\n",
    "    for s in sizes:\n",
    "        counters = Counter()\n",
    "        for basket in baskets:\n",
    "            candidate_items = _get_candidate_items(\n",
    "                freq_items_set, basket, last_freq_iset, s\n",
    "            )\n",
    "            for itemset in combinations(candidate_items, s):\n",
    "                counters[tuple(sorted(itemset))] += 1\n",
    "        new_frequent = [\n",
    "            itemset for itemset, count in counters.items() if count > threshold\n",
    "        ]\n",
    "        if len(new_frequent) == 0:\n",
    "            break\n",
    "        result.extend(new_frequent)\n",
    "        last_freq_iset = new_frequent\n",
    "\n",
    "    return list(result)\n",
    "\n",
    "\n",
    "def PCY(\n",
    "    baskets: Iterable[tuple[str]],\n",
    "    threshold: int,\n",
    "    buckets: int,\n",
    "    iset_len_limit: int | None = None,\n",
    ") -> list[tuple]:\n",
    "    \"\"\"The PCY algorithm\n",
    "\n",
    "    Args:\n",
    "        baskets (Iterable[tuple[str]]): the baskets\n",
    "        threshold (int): the threshold over which an itemset is frequent\n",
    "        buckets (int): the number of buckets to use for counting pairs\n",
    "        iset_len_limit (int | None, optional): the maximum cardinality of itemsets to consider. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple]: frequent itemsets, including false positives\n",
    "    \"\"\"\n",
    "    item_counts = Counter()\n",
    "    itemset_counts = np.zeros(buckets, dtype=np.uint32)\n",
    "\n",
    "    for basket in baskets:\n",
    "        for item in basket:\n",
    "            item_counts[item] += 1\n",
    "        for itemset in combinations(basket, 2):\n",
    "            itemset_counts[hash_t(itemset) % buckets] += 1\n",
    "\n",
    "    freq_items = [item for item, count in item_counts.items() if count > threshold]\n",
    "    del item_counts\n",
    "    gc.collect()\n",
    "\n",
    "    bitmap = Bitmap([count > threshold for count in itemset_counts])\n",
    "    del itemset_counts\n",
    "    gc.collect()\n",
    "\n",
    "    freq_couples = [\n",
    "        tuple(sorted(itemset))\n",
    "        for itemset in combinations(freq_items, 2)\n",
    "        if bitmap.get(hash_t(itemset) % buckets)\n",
    "    ]\n",
    "\n",
    "    return apriori(baskets, threshold, freq_items, freq_couples, iset_len_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, the itemsets with cardinality larger than 2 are obtained using the apriori algorithm, as a low number of those itemset is expected to be frequent.\n",
    "\n",
    "PCY alone can be used to retrieve frequent itemset (buckets) using a single node for computation. To avoid long execution times, in this section only frequent couples will be computed. Unsetting the `iset_len_limit` optional parameter in the cell below (or setting it to an higher value) will force the search to look for frequent itemsets with higher cardinality. The next cell should take around 7 minutes in a Google Colab CPU runtime.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = PCY(Baskets(\"baskets\"), 100, int(1e8))\n",
    "len(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result contains also false positive couples, that is infrequent couples put in frequent buckets. To remove false positives, another pass is needed: we count occurrences of pairs in baskets and we discard the ones that don't reach a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrences(\n",
    "    baskets: Iterable[tuple],\n",
    "    candidates: Iterable[tuple],\n",
    ") -> list[tuple[tuple, int]]:\n",
    "    \"\"\"Count occurrences of candidates in baskets\n",
    "\n",
    "    Args:\n",
    "        baskets (Iterable[tuple]): the baskets\n",
    "        candidates (Iterable[tuple]): the candidate itemsets\n",
    "        iset_len_limit (int | None, optional): the maximum cardinality of itemsets to consider. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[tuple, int]]: a list of itemsets with the occurrences\n",
    "    \"\"\"\n",
    "    counts = Counter(\n",
    "        itemset\n",
    "        for basket in baskets\n",
    "        for itemset in candidates\n",
    "        if frozenset(basket).issuperset(itemset)\n",
    "    )\n",
    "    return list(counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent = [\n",
    "    x\n",
    "    for x in sorted(\n",
    "        count_occurrences(Baskets(\"baskets\"), candidates), key=lambda x: -x[1]\n",
    "    )\n",
    "    if x[1] > 100\n",
    "]\n",
    "\n",
    "with open(\"PCY_frequent_itemsets.json\", \"w\") as f:\n",
    "    json.dump([{\"set\": s[0], \"count\": s[1]} for s in frequent], f, indent=2)\n",
    "\n",
    "print(\"Number of frequent itemsets:\", len(frequent))\n",
    "print(\"List of frequent itemsets saved in 'PCY_frequent_itemsets.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SON\n",
    "\n",
    "Execution times can be improved by using SON, parallelizing the execution of PCY on a number of chunks and combining the results. The Apache Spark framework is used for the implementation of the SON algorithm. \n",
    "\n",
    "We start by loading the basket files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"baskets\")\n",
    "baskets: RDD = df.rdd.map(lambda row: tuple(json.loads(row.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory usage\n",
    "\n",
    "The objective is to have the maximum memory usage without swapping and thus thrashing. The main elements to store in memory are:\n",
    "- The hash table of item counters\n",
    "- The array of bucket counters\n",
    "- The bitmap of frequent buckets\n",
    "\n",
    "The memory usage of the bitmap and the array of counters is easy to predict given the size, more tricky is doing it for the hash table of counters. For this purpose, we use a tool for observing memory behaviour of Python objects, namely Pympler. The `asizeof` method return an approximation of the memory usage of an object.\n",
    "\n",
    "We measure the size of the `Counter` object after counting all items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(baskets.flatMap(lambda x: x).collect())\n",
    "print(\"{0:.3f} MB\".format(asizeof(counter) / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can assume that a single node won't use more than 200 MB for storing the item counters. The remaining space can be used to store the bucket counters. Assuming we want to use up to 2 GB of memory for each computing node, we can use a number of buckets with 32-bit unsigned integer counters equal to:\n",
    "$$\n",
    "\\frac{2 \\cdot 10^9 \\text{ B} - 2 \\cdot 10^8 \\text{ B}}{4 \\text{ B}} = 4.5 \\cdot 10^8 \\text{ buckets}\n",
    "$$\n",
    "\n",
    "while the bitmap will occupy $1/32$ of the space, that is $56 \\text{ MB}$.\n",
    "\n",
    "The last parameters to tune are the thresholds for labelling an itemset as frequent. This will be chosen experimentally in a way to obtain an output of reasonable size, say around 50 itemsets in the whole basket list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-reduce implementation\n",
    "\n",
    "To avoid long times in resource constrained environments, only a randomly sampled list of baskets is used during the analysis. For the same reason, the used number of bucket will be smaller than the one computed above.\n",
    "\n",
    "The result of the analysis on the original dataset using $10^7$ buckets and a threshold of 100 is reported in the `frequent_itemset.json` file in the repository. The next cell shows an execution on a much smaller set of baskets, considering only couples. To search from larger baskets and to retrieve itemsets with larger cardinality, simply increase respectively the `SAMPLE_FRACTION` and `ISET_LEN_LIMIT` constants, possibily setting to `None` the latter to remove the cardinality limit.\n",
    "\n",
    "The cell below takes around 8 minutes to generate a result in a Google Colab CPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKETS = int(1e7)\n",
    "THRESHOLD = 30\n",
    "ISET_LEN_LIMIT = 2\n",
    "SAMPLE_FRACTION = .2\n",
    "CHUNKS = 5\n",
    "\n",
    "if SAMPLE_FRACTION < 1:\n",
    "    baskets = baskets.sample(False, SAMPLE_FRACTION, 2)\n",
    "\n",
    "baskets.repartition(CHUNKS)\n",
    "\n",
    "def SON(baskets: RDD, threshold: int, buckets: int) -> list[tuple]:\n",
    "    \"\"\"The SON algorithm\n",
    "\n",
    "    Args:\n",
    "        baskets (RDD): The Spark RDD containing baskets\n",
    "        threshold (int): the threshold over which an itemset is frequent\n",
    "        buckets (int): the number of buckets to use for counting pairs\n",
    "\n",
    "    Returns:\n",
    "        list[tuple]: the frequent itemsets\n",
    "    \"\"\"\n",
    "    num_chunks = baskets.getNumPartitions()\n",
    "    candidates = (\n",
    "        baskets.mapPartitions(\n",
    "            lambda chunk: PCY(\n",
    "                list(chunk), threshold // num_chunks, buckets, ISET_LEN_LIMIT\n",
    "            )\n",
    "        )\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "    frequent_itemsets = (\n",
    "        baskets.mapPartitions(\n",
    "            lambda chunk: count_occurrences(list(chunk), candidates)\n",
    "        )\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .filter(lambda x: x[1] > THRESHOLD)\n",
    "        .collect()\n",
    "    )\n",
    "    frequent_itemsets.sort(key=lambda x: -x[1])\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "frequent_itemsets = SON(baskets, THRESHOLD, BUCKETS)\n",
    "\n",
    "for x in frequent_itemsets[:5]:\n",
    "    print(x)\n",
    "with open(\"SON_frequent_itemsets.json\", \"w\") as f:\n",
    "    json.dump([{\"set\": s[0], \"count\": s[1]} for s in frequent_itemsets], f, indent=2)\n",
    "\n",
    "print(\"Number of frequent itemsets:\", len(frequent_itemsets))\n",
    "print(\"List of frequent itemsets saved in 'SON_frequent_itemsets.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
