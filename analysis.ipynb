{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "! kaggle datasets download -f actors.csv gsimonx37/letterboxd\n",
    "! unzip actors.csv.zip\n",
    "! rm actors.csv.zip\n",
    "\n",
    "# get whole repo if running in google colab\n",
    "! git clone https://github.com/mattia01017/movie-actor-mb-analysis\n",
    "! pip install -r movie-actor-mb-analysis/requirements.txt\n",
    "\n",
    "# setup Spark\n",
    "import os\n",
    "import findspark\n",
    "! apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "! wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
    "! tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
    "! pip install -q findspark\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "findspark.init(\"spark-3.1.1-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from pympler.asizeof import asizeof\n",
    "from typing import Iterable\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "from dotenv import load_dotenv\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market-basket analysis of Letterboxd dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Starting from a csv table that associate film identifiers to actors, we want to have on disk a list of baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)\n",
    "reader = csv.reader(open(\"actors.csv\"))\n",
    "next(reader)\n",
    "\n",
    "for row in reader:\n",
    "    data[row[0]] = [row[1]]\n",
    "\n",
    "with open(\"baskets.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(basket) for basket in data.values()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an iterator that implement a lazy loading of file data. In this way, we can load in memory a basket at a time instead of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baskets(Iterable):\n",
    "    def __init__(self, filename, stop: int | None = None) -> None:\n",
    "        self.filename = filename\n",
    "        self.stop = stop\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.file = open(self.filename)\n",
    "        self.read = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.file.closed: raise StopIteration\n",
    "        line = self.file.readline()\n",
    "        if not line or self.read == self.stop: \n",
    "            self.file.close()\n",
    "            raise StopIteration\n",
    "        self.read += 1\n",
    "        return tuple(json.loads(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "For the analysis, the Savasere, Omiecinski and Navathe (SON) algorithm will be implemented using the Park, Cheng and Yu (PCY) algorithm for the chunks.\n",
    "\n",
    "### PCY\n",
    "\n",
    "First we define a very simple class representing a bitmap useful for the PCY algorithm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bitmap:\n",
    "    def __init__(self, bits_arr: list) -> None:\n",
    "        self.bytes = np.packbits(bits_arr, bitorder=\"little\")\n",
    "\n",
    "    def get(self, index: int) -> bool:\n",
    "        return bool(self.bytes[index // 8] & pow(2, index % 8))\n",
    "\n",
    "    def set(self, index: int):\n",
    "        self.bytes[index // 8] |= pow(2, index % 8)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \" \".join([\"{0:08b}\".format(b) for b in self.bytes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, the PCY algorithm is implemented. The garbage collector is manually triggered for deleting from memory the counters after the bitmap creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hash_t(itemset: tuple) -> int:\n",
    "    \"\"\"hash tuple ignoring order\"\"\"\n",
    "    return reduce(lambda p, c: p ^ hash(c), itemset, 0)\n",
    "\n",
    "def pcy(\n",
    "    baskets: Iterable[tuple[str]],\n",
    "    threshold: int,\n",
    "    buckets: int,\n",
    ") -> list[tuple]:\n",
    "    item_counts = Counter()\n",
    "    itemset_counts = np.zeros(buckets, dtype=np.uint32)\n",
    "\n",
    "    for basket in baskets:\n",
    "        for item in basket:\n",
    "            item_counts[item] += 1\n",
    "        for itemset in combinations(basket, 2):\n",
    "            itemset_counts[hash_t(itemset) % buckets] += 1\n",
    "            \n",
    "            \n",
    "    bitmap = Bitmap([count > threshold for count in itemset_counts])\n",
    "    del itemset_counts\n",
    "    gc.collect()\n",
    "\n",
    "    freq_items = [item for item, count in item_counts.items() if count > threshold]\n",
    "    del item_counts\n",
    "    gc.collect()\n",
    "\n",
    "    return [\n",
    "        itemset\n",
    "        for itemset in combinations(freq_items, 2)\n",
    "        if bitmap.get(hash_t(itemset) % buckets)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCY alone can be used to retrieve the frequent itemsets using a single node for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcy(Baskets(\"baskets.txt\"), 100, int(1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result contains also false positive, that is infrequent itemset put in a frequent bucket. To remove false positives, another pass would be needed. The topic is best delved in SON implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SON\n",
    "\n",
    "Execution times can be improved by using SON, parallelizing the execution of PCY on a number of chunks and combining the results. The Apache Spark framework is used for the implementation of the SON algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/03 17:04:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "CHUNKS = 5\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"movie-actor-mb-analysis\")\\\n",
    "    .config(\"spark.default.parallelism\", str(CHUNKS))\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import the dataset from the CSV file. To get a convenient representation of data, rows will be grouped by film ID to obtain a Spark dataframe with a basket for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"actors.csv\", header=True, sep=\",\", mode=\"DROPMALFORMED\")\n",
    "baskets = df.rdd\\\n",
    "    .map(lambda x: (x[\"id\"], x[\"name\"]))\\\n",
    "    .groupByKey()\\\n",
    "    .map(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "603162"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baskets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures size\n",
    "\n",
    "The objective is to have the maximum memory usage without swapping and thus thrashing. The main elements to store in memory are:\n",
    "- The hash table of item counters\n",
    "- The array of bucket counters\n",
    "- The bitmap of frequent buckets\n",
    "\n",
    "The memory usage of the bitmap and the array of counters is easy to predict given the size, more tricky is the hash table of counters. For this purpose, we use a tool for observing memory behaviour of Python objects, namely Pympler. The `asizeof` method return an approximation of the memory usage of an object.\n",
    "\n",
    "We measure the size of the `Counter` object after counting all items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(baskets.flatMap(lambda x: x).collect())\n",
    "print(\"{0:.3f} MB\".format(asizeof(counter) / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can assume that a single node won't use more than 200 MB for storing the item counters. The remaining space can be used to store the bucket counters. Assuming we want to use up to 2 GB of memory for each computing node, we can use a number of buckets with 32-bit unsigned integer counters equal to:\n",
    "$$\n",
    "\\frac{2 \\cdot 10^9 \\text{ B} - 2 \\cdot 10^8 \\text{ B}}{4 \\text{ B}} = 4.5 \\cdot 10^8 \\text{ buckets}\n",
    "$$\n",
    "\n",
    "while the bitmap will occupy $1/32$ of the space.\n",
    "\n",
    "The last parameters to tune are the thresholds for labelling a bucket in a chunk and an itemset in the whole dataset as frequent. Those will be chosen experimentally in a way to obtain an output of reasonable size, say around 50 itemsets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-reduce implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Larry Fine', 'Moe Howard'), 235)\n",
      "(('Jack Mercer', 'Mae Questel'), 167)\n",
      "(('Ali Basha', 'Brahmanandam'), 159)\n",
      "(('Oliver Hardy', 'Stan Laurel'), 150)\n",
      "(('Bebe Daniels', 'Harold Lloyd'), 150)\n",
      "(('Harold Lloyd', \"Harry 'Snub' Pollard\"), 148)\n",
      "(('Bebe Daniels', \"Harry 'Snub' Pollard\"), 148)\n",
      "(('James Hetfield', 'Lars Ulrich'), 148)\n",
      "(('Kirk Hammett', 'Lars Ulrich'), 146)\n",
      "(('James Hetfield', 'Kirk Hammett'), 145)\n",
      "Number of candidates: 313\n",
      "Number of actual frequent itemsets: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "BUCKETS = int(1e7)\n",
    "THRESHOLD = 100\n",
    "\n",
    "def count_occurrences(baskets, candidates):\n",
    "    out = {tuple(sorted(k)):0 for k in candidates}\n",
    "    for basket in baskets:\n",
    "        for itemset in combinations(basket, 2):\n",
    "            itemset = tuple(sorted(itemset))\n",
    "            if itemset in out:\n",
    "                out[itemset] += 1\n",
    "    return out.items()\n",
    "\n",
    "\n",
    "candidates = baskets\\\n",
    "    .mapPartitions(lambda chunk: pcy(chunk, THRESHOLD // CHUNKS, BUCKETS))\\\n",
    "    .distinct()\\\n",
    "    .collect()\n",
    "\n",
    "frequent_itemsets = baskets\\\n",
    "    .mapPartitions(lambda chunk: count_occurrences(chunk, candidates))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .filter(lambda x:  x[1] > THRESHOLD)\\\n",
    "    .collect()\n",
    "\n",
    "frequent_itemsets.sort(key=lambda x: -x[1])\n",
    "\n",
    "for x in frequent_itemsets[:10]: print(x)\n",
    "with open(\"frequent_itemsets.json\", \"w\") as f:\n",
    "    json.dump(frequent_itemsets, f, indent=2)\n",
    "\n",
    "print(\"Number of candidates:\", len(candidates))\n",
    "print(\"Number of actual frequent itemsets:\", len(frequent_itemsets))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
