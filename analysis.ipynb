{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "! kaggle datasets download -f actors.csv gsimonx37/letterboxd\n",
    "! unzip actors.csv.zip\n",
    "! rm actors.csv.zip\n",
    "\n",
    "# get whole repo if running in google colab\n",
    "! git clone https://github.com/mattia01017/movie-actor-mb-analysis\n",
    "! pip install -r movie-actor-mb-analysis/requirements.txt\n",
    "\n",
    "# setup Spark\n",
    "import os\n",
    "import findspark\n",
    "! apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "! wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
    "! tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
    "! pip install -q findspark\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "findspark.init(\"spark-3.1.1-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from pympler.asizeof import asizeof\n",
    "from typing import Iterable\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market-basket analysis of Letterboxd dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Starting from a csv table that associate film identifiers to actors, we want to have on disk a list of baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)\n",
    "reader = csv.reader(open(\"actors.csv\"))\n",
    "next(reader)\n",
    "\n",
    "for row in reader:\n",
    "    data[row[0]] = [row[1]]\n",
    "\n",
    "with open(\"baskets.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(basket) for basket in data.values()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an iterator that implement a lazy loading of file data. In this way, we can lazily load in memory a basket at a time instead of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baskets(Iterable):\n",
    "    def __init__(self, filename, stop: int | None = None) -> None:\n",
    "        self.filename = filename\n",
    "        self.stop = stop\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.file = open(self.filename)\n",
    "        self.read = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.file.closed: raise StopIteration\n",
    "        line = self.file.readline()\n",
    "        if not line or self.read == self.stop: \n",
    "            self.file.close()\n",
    "            raise StopIteration\n",
    "        self.read += 1\n",
    "        return tuple(json.loads(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms implementation\n",
    "\n",
    "For the analysis, the Savasere, Omiecinski and Navathe (SON) algorithm will be implemented using the Park, Cheng and Yu (PCY) algorithm for the chunks.\n",
    "\n",
    "### PCY\n",
    "\n",
    "First we define a very simple bitmap class useful for the PCY algorithm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bitmap:\n",
    "    def __init__(self, bits_arr: list) -> None:\n",
    "        self.bytes = np.packbits(bits_arr)\n",
    "\n",
    "    def get(self, index: int) -> bool:\n",
    "        return bool(self.bytes[index // 8] & pow(2, index % 8))\n",
    "\n",
    "    def set(self, index: int):\n",
    "        self.bytes[index // 8] |= pow(2, index % 8)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \" \".join([\"{0:08b}\".format(b) for b in self.bytes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, the PCY algorithm is implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcy(\n",
    "    baskets: Iterable[tuple[str]],\n",
    "    threshold: int,\n",
    "    buckets: int,\n",
    ") -> list[tuple]:\n",
    "    item_counts = Counter()\n",
    "    itemset_counts = np.zeros(buckets, dtype=np.uint32)\n",
    "\n",
    "    for basket in baskets:\n",
    "        for item in basket:\n",
    "            item_counts[item] += 1\n",
    "        for itemset in combinations(basket, 2):\n",
    "            itemset_counts[hash(itemset) % buckets] += 1\n",
    "\n",
    "    freq_items = [item for item, count in item_counts.items() if count > threshold]\n",
    "    del item_counts\n",
    "\n",
    "    bitmap = Bitmap([count > threshold for count in itemset_counts])\n",
    "    del itemset_counts\n",
    "\n",
    "    return [\n",
    "        itemset\n",
    "        for itemset in combinations(freq_items, 2)\n",
    "        if bitmap.get(hash(itemset) % buckets)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCY alone can be used to retrieve the frequent itemsets using a single node for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mel Blanc', 'Jiří Lábus'),\n",
       " ('James Hetfield', 'Lars Ulrich'),\n",
       " ('Saburo Date', 'Rajendran'),\n",
       " ('Larry Fine', 'Moe Howard')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcy(Baskets(\"baskets.txt\"), 150, int(1e9) * 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SON\n",
    "\n",
    "Execution times can be improved by using SON, parallelizing the execution of PCY on a number of chunks and combining the results. The Apache Spark framework is used for the implementation of the SON algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/03 15:30:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "CHUNKS = 5\n",
    "# Assuming we have 5 computing nodes\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"movie-actor-mb-analysis\")\\\n",
    "    .config(\"spark.default.parallelism\", str(CHUNKS))\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import the dataset from the CSV file. To get a convenient representation of data, rows will be grouped by film ID to obtain a Spark dataframe with a basket for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"actors.csv\", header=True, sep=\",\", mode=\"DROPMALFORMED\")\n",
    "baskets = df.rdd\\\n",
    "    .map(lambda x: (x[\"id\"], x[\"name\"]))\\\n",
    "    .groupByKey()\\\n",
    "    .map(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "603162"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baskets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures size\n",
    "\n",
    "The objective is to have the maximum memory usage without swapping and thus thrashing. The main elements to store in memory are:\n",
    "- The hash table of item counters\n",
    "- The array of bucket counters\n",
    "- The bitmap of frequent buckets\n",
    "\n",
    "The memory usage of the bitmap and the array of counters is easy to predict given the size, more tricky is the hash table of counters. For this purpose, we use a tool for observing memory behaviour of Python objects, namely Pympler. The `asizeof` method return an approximation of the memory usage of an object.\n",
    "\n",
    "We measure the size of the `Counter` object after counting all items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.714 MB\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(baskets.flatMap(lambda x: x).collect())\n",
    "print(\"{0:.3f} MB\".format(asizeof(counter) / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can assume that a single node won't use more than 200 MB for storing the item counters. The remaining space can be used to store the bucket counters. Assuming we want to use up to 2 GB of memory for each computing node, we can use a number of buckets with 32-bit unsigned integer counters equal to:\n",
    "$$\n",
    "\\frac{2 \\cdot 10^9 \\text{ B} - 2 \\cdot 10^8 \\text{ B}}{4 \\text{ B}} = 4.5 \\cdot 10^8 \\text{ buckets}\n",
    "$$\n",
    "\n",
    "The last parameter to tune is the threshold for labelling a bucket as frequent in a chunk. This will be chosen in an experimental way by fixing the output to a reasonable size, say around 50 itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-reduce implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 15:31:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 2:>                                                          (0 + 5) / 5]\r"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 40\n",
    "BUCKETS = int(1e6)\n",
    "\n",
    "def count_occurrences(baskets, candidates):\n",
    "    out = {tuple(k):0 for k in candidates}\n",
    "    for basket in baskets:\n",
    "        for itemset in combinations(basket, 2):\n",
    "            if itemset in out:\n",
    "                out[itemset] += 1\n",
    "    return out.items()\n",
    "\n",
    "\n",
    "candidates = baskets\\\n",
    "    .mapPartitions(lambda chunk: pcy(chunk, THRESHOLD // CHUNKS, BUCKETS))\\\n",
    "    .distinct()\\\n",
    "    .collect()\n",
    "\n",
    "frequent_itemsets = baskets\\\n",
    "    .mapPartitions(lambda chunk: count_occurrences(chunk, candidates))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .filter(lambda x:  x[1] > THRESHOLD)\\\n",
    "    .collect()\n",
    "    \n",
    "frequent_itemsets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
